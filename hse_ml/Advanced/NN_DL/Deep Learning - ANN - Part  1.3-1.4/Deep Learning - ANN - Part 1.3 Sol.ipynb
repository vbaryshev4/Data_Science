{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Глава 1. Полносвязные нейронные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 3. Примеры реальных задач"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Три финансовые задачи:\n",
    "1. Задача классификации (мошенничества с кредитными картами)\n",
    "2. Задача регрессии (страховые потери)\n",
    "3. Задача обучения без учителя с применением автоэнкодера (мошенничества с кредитными картами)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Задача классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Набор данных, который мы собираемся использовать, можно скачать с Kaggle. \n",
    "\n",
    "* Он содержит данные об операциях с кредитными картами, которые произошли в течение двух дней, с 492 мошенничествами из 284 807 транзакций.\n",
    "\n",
    "* Все переменные в наборе данных являются числовыми.\n",
    "\n",
    "* Данные были преобразованы с использованием преобразования PCA по соображениям конфиденциальности. \n",
    "\n",
    "* Два признака, которые не были преобразованы, это Время и Сумма транзакции. Время содержит секунды, прошедшие между каждой транзакцией и первой транзакцией в наборе данных."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "rcParams['figure.figsize'] = 14, 8\n",
    "\n",
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LABELS = [\"Normal\", \"Fraud\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_classes = pd.value_counts(df['Class'], sort = True)\n",
    "count_classes.plot(kind = 'bar', rot=0)\n",
    "plt.title(\"Transaction class distribution\")\n",
    "plt.xticks(range(2), LABELS)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frauds = df[df.Class == 1]\n",
    "normal = df[df.Class == 0]\n",
    "frauds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frauds.Amount.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(20,10))\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "f.suptitle('Amount per transaction by class')\n",
    "\n",
    "bins = 50\n",
    "\n",
    "ax1.hist(frauds.Amount, bins = bins)\n",
    "ax1.set_title('Fraud')\n",
    "\n",
    "ax2.hist(normal.Amount, bins = bins)\n",
    "ax2.set_title('Normal')\n",
    "\n",
    "plt.xlabel('Amount ($)')\n",
    "plt.ylabel('Number of Transactions')\n",
    "plt.xlim((0, 20000))\n",
    "plt.yscale('log')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "f.suptitle('Time of transaction vs Amount by class')\n",
    "\n",
    "ax1.scatter(frauds.Time, frauds.Amount)\n",
    "ax1.set_title('Fraud')\n",
    "\n",
    "ax2.scatter(normal.Time, normal.Amount)\n",
    "ax2.set_title('Normal')\n",
    "\n",
    "plt.xlabel('Time (in Seconds)')\n",
    "plt.ylabel('Amount')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size = 0.2, stratify=df.Class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train.iloc[:,1:30].values[:]\n",
    "y_train = train['Class'].values[:]\n",
    "\n",
    "x_test = test.iloc[:,1:30].values[:]\n",
    "y_test = test['Class'].values[:]\n",
    "\n",
    "print (x_train.shape, y_train.shape)\n",
    "print (x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "x_train = sc.fit_transform(x_train)\n",
    "x_test = sc.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Функция активации ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU является линейным для всех положительных значений и нулем для всех отрицательных значений. Это означает, что:\n",
    "\n",
    "* Просто с вычислительной точки зрения. Таким образом, модели может потребоваться меньше времени для обучения.\n",
    "\n",
    "* Быстрее сходимость. Линейность означает, что наклон не является плато или промежутком насыщения, когда x становится большим. \n",
    "\n",
    "* У ReLu нет проблемы исчезающего градиента, от которой страдают другие функции активации, такие как сигмоид или гиперболический тангес.\n",
    "\n",
    "* Поскольку ReLU равен нулю для всех отрицательных входов. Производная также равна нулю. Бороться с этой проблемой, мы можем использовать негерметичный ReLU. Leaky ReLU гарантирует, что наклон для отрицательных значений не равен нулю."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/ann20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Прореживание нейронной сети Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Переобучение (overfitting)** — одна из проблем глубоких нейронных сетей (Deep Neural Networks, DNN), состоящая в следующем: модель хорошо объясняет только примеры из обучающей выборки, адаптируясь к обучающим примерам, вместо того чтобы учиться классифицировать примеры, не участвовавшие в обучении (теряя способность к обобщению). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Главная идея Dropout — вместо обучения одной DNN обучить ансамбль нескольких DNN, а затем усреднить полученные результаты.\n",
    "\n",
    "* Сети для обучения получаются с помощью исключения из сети (dropping out) нейронов с вероятностью , таким образом, вероятность того, что нейрон останется в сети, составляет . “Исключение” нейрона означает, что при любых входных данных или параметрах он возвращает 0.\n",
    "\n",
    "* Исключенные нейроны не вносят свой вклад в процесс обучения ни на одном из этапов алгоритма обратного распространения ошибки (backpropagation); поэтому исключение хотя бы одного из нейронов равносильно обучению новой нейронной сети.\n",
    "\n",
    "* В двух словах, Dropout хорошо работает на практике, потому что предотвращает взаимоадаптацию нейронов на этапе обучения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/ann19.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Методы оптимизации нейронных сетей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://habr.com/ru/post/318970/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Стохастический градиентный спуск (SGD)\n",
    "\n",
    "* Nesterov Accelerated Gradient\n",
    "\n",
    "* Adagrad\n",
    "\n",
    "* RMSPROP и Adadelta\n",
    "\n",
    "* Adam "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Я бы предложил держать качестве метода оптимизации по умолчанию **Adam**, потому что он выдаёт наилучшие результаты при минимальном подгоне параметров. \n",
    " \n",
    "* Когда сеть уже более-менее отлажена, можно попробовать метод Нестерова с разными параметрами. Иногда с помощью него можно добиться лучших результатов, но он сравнительно чувствителен к изменениям в сети. Плюс-минус пара слоёв и нужно искать новый оптимальный learning rate.\n",
    "\n",
    "* Рассматривайте остальные алгоритмы и их параметры как ещё несколько ручек и тумблеров, которые можно подёргать в каких-то специальных случаях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Бинарная кросс-энтропия или Log Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/ann21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/ann22.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Описание доступных функций потерь в Keras: https://keras.io/losses/\n",
    "Как добавить свою метрику качества?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Keras Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вызываемые функции в Keras\n",
    "\n",
    "* **ModelCheckpoint**\n",
    "\n",
    "Эта вызываемая функция сохранит вашу модель в виде файла контрольной точки (в формате hdf5) на диск после каждой успешной эпохи. Вы можете фактически установить выходной файл с динамическим именем в зависимости от эпохи. Вы также можете записать либо значение потерь, либо значение точности как часть имени файла журнала.\n",
    "\n",
    "* **CSVLogger**\n",
    "\n",
    "CSVLogger записывает файл истории в CSV, содержащий информацию об эпохах, точности и потерях на диск, чтобы вы могли проверить его позже.\n",
    "\n",
    "* **EarlyStopping**\n",
    "\n",
    "Одним из способов преодтвращения переобучения в нейронных сетях является использование ранней остановки. Ранняя остановка предотвращает переобучение вашей модели, прерывая процесс обучения, если процесс обучения более не улучшает обоющающую способность нейронной сети.\n",
    "\n",
    "* **RemoteMonitor**\n",
    "\n",
    "Этот возвращающая функция отправляет сообщения о состоянии в виде JSON через HTTP POST. Это может быть легко интегрировано со службой обмена сообщениями или с очередью, например Kafka, Amazon SQS и др.\n",
    "\n",
    "* **LearningRateScheduler**\n",
    "\n",
    "Другой метод оптимизации с глубоким обучением состоит в том, чтобы регулировать скорость обучения с течением времени. Скорость обучения определяет размером шагов градиентного спуска.\n",
    "\n",
    "Один из методов - начать с относительно большого значения и уменьшать с увеличением эпох обучения. Все, что вам нужно сделать, это написать простую функцию, которая возвращает желаемую скорость обучения на основе текущей эпохи и передает ее в качестве параметра по расписанию (параметр schedule в callback LearningRateScheduler).\n",
    "\n",
    "* **Tensorboard**\n",
    "\n",
    "Это, пожалуй, самый крутой из всех стандартных callbacks. Используя callback TensorBoard, журналы будут записываться в каталог, который вы затем сможете просмотреть с помощью инструмента визуализации TensorBoard: кривые обучения, метрики качества и др.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(128, activation='relu', input_shape=(29,)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.summary()\n",
    "model.compile(optimizer='SGD',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=500,\n",
    "                    epochs=50,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict_proba(x_test)\n",
    "y_pred = model.predict(x_test)\n",
    "y_pred = (y_pred.flatten() > 0.5)*1\n",
    "result_df = pd.DataFrame({'proba': predictions.flatten(), 'true_class': y_test})\n",
    "result_df[result_df[\"true_class\"]==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4. Кривая ROC. Площадь под ROC кривой - AUC ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,\n",
    "                             roc_curve, recall_score, classification_report, f1_score,\n",
    "                             precision_recall_fscore_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(result_df.true_class, result_df.proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.001, 1])\n",
    "plt.ylim([0, 1.001])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5. Кривая точность-полнота (Precision-recall curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/ann15.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/ann17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/ann18.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, th = precision_recall_curve(result_df.true_class, result_df.proba)\n",
    "plt.plot(recall, precision, 'b', label='Precision-Recall curve')\n",
    "plt.title('Recall vs Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Задача регрессии "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предсказание страхового требования к Страховой компании AllState\n",
    "\n",
    "Каждая строка в этом наборе данных представляет страховое требование. Вы должны предсказать значение для столбца «потери». Переменные, начинающиеся с 'cat', являются категориальными, тогда как переменные, начинающиеся с 'cont', являются непрерывными."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/c/allstate-claims-severity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "def prepare_data(data, train=True, dv=None):\n",
    "\n",
    "    cat_keys = [k for k in data.keys() if k.startswith(\"cat\")]\n",
    "    cat_x = data[cat_keys]\n",
    "    cont_keys = [k for k in data.keys() if k.startswith(\"cont\")]\n",
    "    cont_x = data[cont_keys]\n",
    "    if train:\n",
    "        y = data[\"loss\"]\n",
    "    else:\n",
    "        y = None\n",
    "    cat_x_dict = [r[1].to_dict() for r in cat_x.iterrows()]\n",
    "    del cat_x\n",
    "    if dv is None:\n",
    "        dv = DictVectorizer().fit(cat_x_dict)\n",
    "    cat_cont_x = dv.transform(cat_x_dict).toarray()\n",
    "    del cat_x_dict\n",
    "    return np.column_stack([cat_cont_x, cont_x]), y, dv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"datasets/train.csv\").set_index(\"id\")\n",
    "train_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of missing values\", train_data.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Целевая переменная перекошена (ассиметрия)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,9))\n",
    "sns.distplot(train_data[\"loss\"])\n",
    "sns.boxplot(train_data[\"loss\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выглядит так, что распределение мы смогли нормализовать. Таким образом, мы будем использовать логарифм целевой переменной, потому что это дает возможность побороть выбросы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13,9))\n",
    "sns.distplot(np.log1p(train_data[\"loss\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x, train_y, dict_vec = prepare_data(train_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"datasets/test.csv\").set_index(\"id\")\n",
    "test_x, _, _ = prepare_data(test_data, False, dict_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, Dropout\n",
    "from keras.objectives import MSE, MAE\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "seed=2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def baseline_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(input_dim=train_x.shape[1], output_dim=256))\n",
    "    model.add(Activation(\"tanh\"))\n",
    "    model.add(Dropout(0.50))\n",
    "    model.add(Dense(output_dim=128))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.50))\n",
    "    model.add(Dense(output_dim=64))\n",
    "    model.add(Activation(\"relu\"))\n",
    "    model.add(Dropout(0.50))\n",
    "    model.add(Dense(output_dim=1))\n",
    "    model.compile(optimizer=\"Nadam\", loss=\"mean_absolute_error\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10% на валидацию для контроля early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = baseline_model()\n",
    "\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "train_log = model.fit(train_x, train_y.values, batch_size=256, nb_epoch=200, validation_split=0.1, \n",
    "                      verbose=2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(train_log.history[\"loss\"], label=\"loss\")\n",
    "plt.plot(train_log.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_y = model.predict(test_x)\n",
    "result = pd.DataFrame(pred_y, index=test_data.index, columns=[\"loss\"])\n",
    "result.to_csv(\"submission.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Кросс-валидация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed=17\n",
    "np.random.seed(seed)\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasRegressor(build_fn=baseline_model, epochs=5, batch_size=500, verbose=0)))\n",
    "pipeline = Pipeline(estimators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, random_state=seed)\n",
    "results = cross_val_score(pipeline, train_x, train_y, cv=kfold, scoring=\"neg_mean_absolute_error\")\n",
    "print(\"Metrics: %.2f (%.2f) MSE\" % (results.mean(), results.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3. Задача классификации на основе нейросетевого автоэнкодера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Нейросетевой автоэнкодер** – алгоритм машинного обучения без учителя. \n",
    "\n",
    "* Основная идея автоэнкодера заключается в **уменьшении размерности** путем отображения исходного пространства признаков в латентное пространство меньшей размерности (этап кодирования), а затем в реконструкции входных данных на основе латентного представления (этап декодирования). \n",
    "\n",
    "* Сокращая размерность пространства, мы тем самым обучаем модель запоминать только наиболее важную информацию, из которой можно восстановить первоначальные данные. \n",
    "\n",
    "* Это свойство достигается путем формирования структуры кодировщика в виде «горлышка бутылки» (bottleneck), значительного уменьшая размерность последнего скрытого слоя. \n",
    "\n",
    "* Целью автоэнкодера является минимизация ошибки реконструкции между входными и выходными данными. Чтобы уменьшить ошибку реконструкции, в процессе обучения мы движемся в обратном направлении по нейронной сети и обновляем веса.\n",
    "\n",
    "* Веса нейронов обновляются в зависимости от того, насколько они вносят вклад в ошибку реконструкции.\n",
    "\n",
    "* Автоэнкодер можно рассматривать как алгоритм сжатия данных.\n",
    "\n",
    "* Нейросетевой автоэнкодер захватывает нелинейные особенности данных, поэтому имеет преимущество перед PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/ann12.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/ann13.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Ошибка реконструкции"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы оптимизируем параметры нашей модели автоэнкодера таким образом, чтобы функция ошибки - ошибка реконструкции модели был сведен к минимуму. На практике часто используется традиционная среднеквадратичная ошибка:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](img/ann14.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Подготовка данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во-первых, давайте отбросим столбец Time (не собираюсь его использовать) и используем StandardScaler от Scikit на Amount. Масштабирующее устройство удаляет среднее значение и масштабирует значения до единичной дисперсии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "data = df.drop(['Time'], axis=1)\n",
    "data['Amount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение автоэнкодера несколько отличается от того, что мы наблюдали ранее. \n",
    "\n",
    "Допустим, у вас есть набор данных, содержащий множество не мошеннических транзакций. Вы хотите обнаружить любую аномалию в новых транзакциях. Мы создадим эту ситуацию, обучив нашу модель только обычным транзакциям. \n",
    "\n",
    "Резервирование правильного класса в тестовом наборе даст нам возможность оценить производительность нашей модели. Мы зарезервируем 20% наших данных для тестирования:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = train_test_split(data, test_size=0.2, stratify=data.Class, random_state=seed)\n",
    "X_train = X_train[X_train.Class == 0]\n",
    "X_train = X_train.drop(['Class'], axis=1)\n",
    "y_test = X_test['Class']\n",
    "X_test = X_test.drop(['Class'], axis=1)\n",
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Построение модели автоэнкодера"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построение модели\n",
    "Наш Автоэнкодер использует 4 полностью связанных слоя с 14, 7, 7 и 29 нейронами соответственно. Первые два слоя используются для нашего кодера, последние два - для декодера. Кроме того, регуляризация L1 будет использоваться во время обучения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = X_train.shape[1]\n",
    "encoding_dim = 256 # 14\n",
    "\n",
    "input_layer = Input(shape=(input_dim, ))\n",
    "encoder = Dense(encoding_dim, activation=\"tanh\", \n",
    "                activity_regularizer=regularizers.l1(10e-5))(input_layer)\n",
    "encoder = Dense(int(encoding_dim / 2), activation=\"relu\")(encoder)\n",
    "decoder = Dense(int(encoding_dim / 2), activation='tanh')(encoder)\n",
    "decoder = Dense(input_dim, activation='relu')(decoder)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте обучим нашу модель 100 эпохам с размером партии 32 сэмпла и сохраним наиболее эффективную модель в файл. ModelCheckpoint, предоставляемый Keras, действительно удобен для таких задач. Кроме того, результаты обучения будут экспортированы в формате, понятном TensorBoard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "def r2_score(y_true, y_pred):\n",
    "    SS_res =  K.sum(K.square(y_true - y_pred)) \n",
    "    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n",
    "    return ( 1 - SS_res/(SS_tot + K.epsilon()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_epoch = 100\n",
    "batch_size = 32\n",
    "\n",
    "autoencoder.compile(optimizer='adam', \n",
    "                    loss='mean_squared_error', \n",
    "                    metrics=[r2_score])\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath=\"model.h5\",\n",
    "                               verbose=0,\n",
    "                               save_best_only=True)\n",
    "\n",
    "tensorboard = TensorBoard(log_dir='./logs',\n",
    "                          histogram_freq=0,\n",
    "                          write_graph=True,\n",
    "                          write_images=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = autoencoder.fit(X_train, X_train, epochs=nb_epoch, batch_size=batch_size,\n",
    "                          shuffle=True, validation_data=(X_test, X_test), verbose=1, \n",
    "                          callbacks=[checkpointer, tensorboard]).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "autoencoder = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Оценка качества модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = autoencoder.predict(X_test)\n",
    "mse = np.mean(np.power(X_test - predictions, 2), axis=1)\n",
    "error_df = pd.DataFrame({'reconstruction_error': mse,\n",
    "                        'true_class': y_test})\n",
    "error_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "normal_error_df = error_df[(error_df['true_class']== 0) & (error_df['reconstruction_error'] < 10)]\n",
    "_ = ax.hist(normal_error_df.reconstruction_error.values, bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "fraud_error_df = error_df[error_df['true_class'] == 1]\n",
    "_ = ax.hist(fraud_error_df.reconstruction_error.values, bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,\n",
    "                             roc_curve, recall_score, classification_report, f1_score,\n",
    "                             precision_recall_fscore_support)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кривые ROC являются очень полезным инструментом для понимания производительности двоичных классификаторов. Однако наш случай немного необычен. У нас очень несбалансированный набор данных. Тем не менее, давайте посмотрим на нашу кривую ROC:\n",
    "\n",
    "Кривая ROC отображает истинную положительную частоту в сравнении с ложной положительной скоростью в зависимости от различных пороговых значений. По сути, мы хотим, чтобы синяя линия была как можно ближе к верхнему левому углу. В то время как наши результаты выглядят довольно хорошо, мы должны помнить о природе нашего набора данных. РПЦ не выглядит для нас очень полезным. Onward ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(error_df.true_class, error_df.reconstruction_error)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.001, 1])\n",
    "plt.ylim([0, 1.001])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 Кривая точности и полноты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "precision, recall, th = precision_recall_curve(error_df.true_class, error_df.reconstruction_error)\n",
    "plt.plot(recall, precision, 'b', label='Precision-Recall curve')\n",
    "plt.title('Recall vs Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(th, precision[1:], 'b', label='Threshold-Precision curve')\n",
    "plt.title('Precision for different threshold values')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ошибка реконструкции растет, а рекол падает"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(th, recall[1:], 'b', label='Threshold-Recall curve')\n",
    "plt.title('Recall for different threshold values')\n",
    "plt.xlabel('Reconstruction error')\n",
    "plt.ylabel('Recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Прогноз "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы предсказать, является ли новая / невидимая транзакция нормальной или мошеннической, мы рассчитаем ошибку реконструкции из данных транзакции. Если ошибка больше, чем предопределенный порог, мы помечаем ее как мошенническую(так как наша модель должна иметь низкую ошибку при обычных транзакциях). Давайте выберем это значение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "threshold = 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "groups = error_df.groupby('true_class')\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for name, group in groups:\n",
    "    ax.plot(group.index, group.reconstruction_error, marker='o', ms=3.5, linestyle='',\n",
    "            label= \"Fraud\" if name == 1 else \"Normal\")\n",
    "ax.hlines(threshold, ax.get_xlim()[0], ax.get_xlim()[1], colors=\"r\", zorder=100, label='Threshold')\n",
    "ax.legend()\n",
    "plt.title(\"Reconstruction error for different classes\")\n",
    "plt.ylabel(\"Reconstruction error\")\n",
    "plt.xlabel(\"Data point index\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = [1 if e > threshold else 0 for e in error_df.reconstruction_error.values]\n",
    "conf_matrix = confusion_matrix(error_df.true_class, y_pred)\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.ylabel('True class')\n",
    "plt.xlabel('Predicted class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наша модель ловит много мошеннических случаев. Однако есть и особенность. Модель имеет большое количество ложных срабатываний. \n",
    "Однако мы можем гибко варьировать порог и контролировать число обнаруживаемых в зависимости, например, от количества, которые мы можем обработать / проверить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Задание 1.\n",
    "\n",
    "Задача опредления вероятности дефолта (неуплаты долга) по кредитной карте\n",
    "\n",
    "https://www.kaggle.com/uciml/default-of-credit-card-clients-dataset\n",
    "\n",
    "* Постройте классификатор на основе полносвязной нейронной сети для определения вероятности дефолта по кредитной карте (невозврат долга).\n",
    "\n",
    "* Экземляров данных довольно мало, поэтому не нужно упорствовать с увеличением глубины и ширины сети\n",
    "\n",
    "* Обязательное кодирование категориальных признаков\n",
    "\n",
    "* Обязательная стандартизация или масштабирование признаков\n",
    "\n",
    "* Если применяете One Hot Encoding (бинарное кодирование), то обратите внимание на регуляризацию, чтобы предотвратить переобучение\n",
    "\n",
    "* Постройте классификатор на основе автоэнкодера для решения этой же задачи аналогично примеру с мошенничествам по картам\n",
    "\n",
    "Удачи!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
