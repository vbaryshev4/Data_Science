{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# !pip install eli5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/ensemble/weight_boosting.py:29: DeprecationWarning: numpy.core.umath_tests is an internal NumPy module and should not be imported. It will be removed in a future NumPy release.\n",
      "  from numpy.core.umath_tests import inner1d\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import ssl\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from keras.models import Model, Input\n",
    "from keras.layers import Dense, LSTM, Dropout, Embedding, SpatialDropout1D, Bidirectional, concatenate\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import accuracy_score\n",
    "from eli5.lime import TextExplainer\n",
    "import regex as re\n",
    "import numpy as np\n",
    "\n",
    "ssl._create_default_https_context= ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 20news dataset. This may take a few minutes.\n",
      "Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
     ]
    }
   ],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian',\n",
    "              'comp.graphics', 'sci.med']\n",
    "\n",
    "twenty_train = fetch_20newsgroups(\n",
    "    subset='train',\n",
    "    categories=categories,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    "    remove=('headers', 'footers'),\n",
    ")\n",
    "twenty_test = fetch_20newsgroups(\n",
    "    subset='test',\n",
    "    categories=categories,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    "    remove=('headers', 'footers'),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class KerasTextClassifier(BaseEstimator, TransformerMixin):\n",
    "#     '''Wrapper class for keras text classification models that takes raw text as input.'''\n",
    "    def __init__(self, max_words=30000, input_length=100, emb_dim=20, n_classes=4, epochs=5, batch_size=32):\n",
    "        self.max_words = max_words\n",
    "        self.input_length = input_length\n",
    "        self.emb_dim = emb_dim\n",
    "        self.n_classes = n_classes\n",
    "        self.epochs = epochs\n",
    "        self.bs = batch_size\n",
    "        self.model = self._get_model()\n",
    "        self.tokenizer = Tokenizer(num_words=self.max_words+1,\n",
    "        lower=True, split=' ', oov_token=None)\n",
    "    def _get_model(self):\n",
    "        input_text = Input((self.input_length,))\n",
    "        text_embedding = Embedding(input_dim=self.max_words + 2, output_dim=self.emb_dim,\n",
    "        input_length=self.input_length, mask_zero=False)(input_text)\n",
    "        text_embedding = SpatialDropout1D(0.5)(text_embedding)\n",
    "        bilstm = Bidirectional(LSTM(units=32, return_sequences=True, recurrent_dropout=0.5))(text_embedding)\n",
    "        x = concatenate([GlobalAveragePooling1D()(bilstm), GlobalMaxPooling1D()(bilstm)])\n",
    "        x = Dropout(0.7)(x)\n",
    "        x = Dense(512, activation=\"relu\")(x)\n",
    "        x = Dropout(0.6)(x)\n",
    "        x = Dense(512, activation=\"relu\")(x)\n",
    "        x = Dropout(0.5)(x)\n",
    "        out = Dense(units=self.n_classes, activation=\"softmax\")(x)\n",
    "        model = Model(input_text, out)\n",
    "        model.compile(optimizer=\"adam\",\n",
    "        loss=\"sparse_categorical_crossentropy\",\n",
    "        metrics=[\"accuracy\"])\n",
    "        return model\n",
    "    def _get_sequences(self, texts):\n",
    "        seqs = self.tokenizer.texts_to_sequences(texts)\n",
    "        return pad_sequences(seqs, maxlen=self.input_length, value=0)\n",
    "    def _preprocess(self, texts):\n",
    "        return [re.sub(r\"\\d\", \"DIGIT\", x) for x in texts]\n",
    "    def fit(self, X, y):\n",
    "# '''\n",
    "#             Fit the vocabulary and the model.\n",
    "#             :params:\n",
    "#             X: list of texts.\n",
    "#             y: labels.\n",
    "#             '''\n",
    "        self.tokenizer.fit_on_texts(self._preprocess(X))\n",
    "        self.tokenizer.word_index = {e: i for e,i in self.tokenizer.word_index.items() if i <= self.max_words}\n",
    "        self.tokenizer.word_index[self.tokenizer.oov_token] = self.max_words + 1\n",
    "        seqs = self._get_sequences(self._preprocess(X))\n",
    "        self.model.fit(seqs, y, batch_size=self.bs, epochs=self.epochs, validation_split=0.1)\n",
    "    def predict_proba(self, X, y=None):\n",
    "        seqs = self._get_sequences(self._preprocess(X))\n",
    "        return self.model.predict(seqs)\n",
    "    def predict(self, X, y=None):\n",
    "        return np.argmax(self.predict_proba(X), axis=1)\n",
    "    def score(self, X, y):\n",
    "        y_pred = self.predict(X)\n",
    "        return accuracy_score(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Unrecognized keyword arguments: {'oov_token': None}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-73976f2d24a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKerasTextClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-79b63f6e4ca8>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, max_words, input_length, emb_dim, n_classes, epochs, batch_size)\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         self.tokenizer = Tokenizer(num_words=self.max_words+1,\n\u001b[0;32m---> 12\u001b[0;31m         lower=True, split=' ', oov_token=None)\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0minput_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.6/site-packages/keras/preprocessing/text.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, num_words, filters, lower, split, char_level, **kwargs)\u001b[0m\n\u001b[1;32m    147\u001b[0m             \u001b[0mnum_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nb_words'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unrecognized keyword arguments: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_counts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOrderedDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Unrecognized keyword arguments: {'oov_token': None}"
     ]
    }
   ],
   "source": [
    "text_model = KerasTextClassifier(epochs=20, max_words=20000, input_length=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_model.score(twenty_test.data, twenty_test.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_test.data[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = twenty_test.data[2]\n",
    "te = TextExplainer(random_state=42)\n",
    "te.fit(doc, text_model.predict_proba)\n",
    "te.show_prediction(target_names=twenty_train.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te.clf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "te.vec_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
