Озвучено и переведено с [SkLearn](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)

* *criterion* - строковый параметр (опционален). Возможные значения: "gini" или "entropy". Смысл: критерий информативности (как мы будем оценивать, какое пороговое правило будет нам самым полезным в каждой ситуации) 
* *splitter* - строковый параметр (опционален). Как выбирать пороговое правило? Если значение "best", то из всех возможных пороговых правил выберется самое лучшее (долгий, но точный путь обучения). Если "random", то сгенерится случайное подмножество пороговых правил и выберется наилучшее из них (быстрый, но менее точный путь обучения)
* *max_depth* - int or None, optional (default=None). Максимальная глубина, до которой мы разрешаем вырасти дереву. По умолчанию None - никаких ограничений. Влияет на переобучение. Чем больше значение, тем меньше шансов переобучиться (но больше шансов недообучиться)
* *min_samples_split* - int, float, optional (default=2). Минимальное количество объектов, с которого мы продолжаем разбиение данных новыми правилами. Если в какой-то момент количество (или доля от общих, если float) объектов в узле стало меньше этого значения - мы делаем эту вершину терминальной (листом), даже если текущее разбиение не идеально. Чем больше значение, тем меньше шансов переобучиться (но больше шансов недообучиться)
* *min_samples_leaf* - int, float, optional (default=1). Минимальное количество объектов в каждом листе. Смысл: если новый, классифицируемый на тесте, объект попал в терминальную вершину с одним объектом обучающей выборки, то значит ли это, что он принадлежит тому же классу? А если этот объект обучающей выборки был выбросом? Т.о., этим параметром мы также можем регулировать риск переобучения. Чем он больше, тем меньше шансов переобучиться (но больше шансов недообучиться)
* min_weight_fraction_leaf : float, optional (default=0.)
The minimum weighted fraction of the sum total of weights (of all the input samples) required to be at a leaf node. Samples have equal weight when sample_weight is not provided.
* max_features : int, float, string or None, optional (default=None). На какое количество (долю, если float) случайно взятых признаков мы будем смотреть, выбирая новое решающее правило. Нужно ли нам шерстить все пороговые правила по всем признакам каждый раз? Может, лучше взять случайным образом N признаков и искать новое правило только по ним? В точности немного рискуем потерять, зато в скорости явно выиграем! Да и в таком случае больше шансов, что больше признаков задействуется (а значит, больше шансов застраховаться от неправильных предсказаний, а значит, меньше шансов переобучиться). Также есть значение “auto” (max_features=sqrt(n_features)), “sqrt” (max_features=sqrt(n_features)), “log2” (max_features=log2(n_features)), None (max_features=n_features). Внимание: алгоритм имеет право проигнорировать этот параметр, если его недостаточно для генерации как минимум одного порогового правила! 
* *random_state* : int, RandomState instance or None, optional (default=None). Если None, то дерево будет каждый раз случайно. В противном случае псевдослучайно (каждый раз один и тот же "случайный" результат)
* *max_leaf_nodes* : int or None, optional (default=None). Максимальное количество терминальных вершин (листьев). По умолчанию (None) не ограничено
* *min_impurity_decrease* : float, optional (default=0.). Порог прироста информации, ниже которого правила не добавляются в дерево.
* *min_impurity_split* : float, (default=1e-7). Критерий ранней остановки поддерева. Если прирост информации стал меньше заданного параметра, значит, пора остановиться и перестать расти в этом поддереве (перестать дробить выборку новыми правилами)
* *presort* : bool, optional (default=False). Если True, то в начале дерево отсортирует данные по каждому признаку, чтобы быстрее искать пороговые правила. Влияет только на скорость обучения
* (\*)*class_weight* : dict, list of dicts, “balanced” or None, default=None. Вес, назначаемый каждому классу (какому классу отдавать больший приоритет при предсказании). Возможные значения параметра: Словарь {метка_класса: вес}, "balanced" - веса назначаются по исходному соотношению классов в обучающей выборке (чем меньше класса в выборке, тем больше его вес), None - у всех классов равный вес, список словарей (в случае, если каждый объект может принадлежать сразу нескольким классам - multilabel classification)
